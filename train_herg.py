"""
hERG Cardiotoxicity Model - Maximum Accuracy
=============================================

Fine-tunes StereoGNN for hERG inhibition prediction.
Target: As close to 100% AUC as possible.

Techniques used:
- Pretrained transporter backbone
- Focal loss for hard examples
- Class balancing
- Cosine annealing with warm restarts
- Label smoothing
- Test-time augmentation (stereoisomer averaging)
- Ensemble of top checkpoints

Usage:
    python train_herg.py --epochs 150
"""

import argparse
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, confusion_matrix
from sklearn.model_selection import StratifiedKFold
from torch_geometric.data import Data, Batch
from torch_geometric.loader import DataLoader as PyGDataLoader
from tqdm import tqdm
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem, Descriptors

from model import StereoGNN, count_parameters
from featurizer import MoleculeGraphFeaturizer


def get_fingerprint(smiles: str, radius: int = 2, n_bits: int = 1024) -> np.ndarray:
    """Generate Morgan fingerprint + key descriptors."""
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None

    # Morgan fingerprint
    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)
    fp_arr = np.zeros(n_bits)
    DataStructs.ConvertToNumpyArray(fp, fp_arr)

    # Key hERG-relevant descriptors
    desc = [
        Descriptors.MolLogP(mol),           # Lipophilicity
        Descriptors.TPSA(mol),              # Polar surface area
        Descriptors.MolWt(mol) / 500,       # Normalized MW
        Descriptors.NumHDonors(mol) / 5,    # H-bond donors
        Descriptors.NumHAcceptors(mol) / 10, # H-bond acceptors
        Descriptors.NumRotatableBonds(mol) / 10,
        Descriptors.NumAromaticRings(mol) / 5,
        Descriptors.FractionCSP3(mol),      # Saturation
    ]

    return np.concatenate([fp_arr, desc]).astype(np.float32)

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


class FocalLoss(nn.Module):
    """Focal loss for hard example mining with class balancing."""
    def __init__(self, alpha=0.5, gamma=1.0, label_smoothing=0.02, pos_weight=1.3):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.label_smoothing = label_smoothing
        self.pos_weight = pos_weight  # Upweight actives (43% of data)

    def forward(self, inputs, targets):
        # Class-balanced weights
        weights = torch.ones_like(targets, dtype=torch.float32)
        weights[targets == 1] = self.pos_weight

        # Focal loss with class weights
        ce_loss = F.cross_entropy(inputs, targets, reduction='none', label_smoothing=self.label_smoothing)
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * weights * (1 - pt) ** self.gamma * ce_loss

        return focal_loss.mean()


FP_DIM = 1024 + 8  # Morgan bits + descriptors


class HERGClassifier(nn.Module):
    """hERG classifier with StereoGNN backbone + fingerprint fusion."""
    def __init__(self, backbone, hidden_dim=128, fp_dim=FP_DIM, dropout=0.3):
        super().__init__()
        self.backbone = backbone

        # Train all layers
        for param in self.backbone.parameters():
            param.requires_grad = True

        # Fingerprint encoder
        self.fp_encoder = nn.Sequential(
            nn.Linear(fp_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
        )

        # Fused dimension = GNN embedding + FP embedding
        fused_dim = hidden_dim + hidden_dim

        # Classification head on fused features
        self.classifier = nn.Sequential(
            nn.Linear(fused_dim, hidden_dim * 2),
            nn.LayerNorm(hidden_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(hidden_dim // 2, 2),
        )

        # Regression head for pIC50
        self.regressor = nn.Sequential(
            nn.Linear(fused_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 1),
        )

    def forward(self, batch):
        # GNN embedding
        gnn_emb = self.backbone.get_embedding(batch)

        # Fingerprint embedding - reshape from batched graphs
        fp = batch.fp.view(gnn_emb.size(0), -1)  # [batch, fp_dim]
        fp_emb = self.fp_encoder(fp)

        # Fuse
        x = torch.cat([gnn_emb, fp_emb], dim=-1)

        # Predict
        logits = self.classifier(x)
        pic50 = self.regressor(x).squeeze(-1)

        return {'logits': logits, 'pIC50': pic50}


def load_herg_data(data_dir: Path) -> pd.DataFrame:
    """Load CLEAN hERG IC50/Ki data."""
    print("Loading hERG data...")

    # Priority: merged > clean > old
    merged_path = data_dir / 'hERG_merged.csv'
    clean_path = data_dir / 'hERG_clean.csv'
    old_path = data_dir / 'hERG_chembl.csv'

    if merged_path.exists():
        df = pd.read_csv(merged_path)
        print(f"  Using MERGED multi-source data: {merged_path}")
    elif clean_path.exists():
        df = pd.read_csv(clean_path)
        print(f"  Using CLEAN IC50/Ki data: {clean_path}")
    else:
        print(f"  WARNING: Clean data not found, run pull_herg_clean.py first!")
        df = pd.read_csv(old_path)

    print(f"  Total molecules: {len(df)}")
    print(f"  Active (IC50 < 10ÂµM): {(df['active'] == 1).sum()}")
    print(f"  Inactive: {(df['active'] == 0).sum()}")
    if 'pIC50' in df.columns:
        print(f"  pIC50 range: {df['pIC50'].min():.2f} - {df['pIC50'].max():.2f}")

    return df


def create_graph_data(
    smiles: str,
    label: int,
    pic50: float,
    featurizer: MoleculeGraphFeaturizer,
) -> Optional[Data]:
    """Create PyG Data object for hERG prediction with fingerprint."""
    try:
        # Use featurizer with dummy labels (we'll set our own)
        data = featurizer.featurize(smiles, {'DAT': -1, 'NET': -1, 'SERT': -1})
        if data is None:
            return None

        # Get fingerprint
        fp = get_fingerprint(smiles)
        if fp is None:
            return None

        # Set hERG labels
        data.y = torch.tensor([label], dtype=torch.long)
        data.pIC50 = torch.tensor([pic50 if pd.notna(pic50) else float('nan')], dtype=torch.float32)
        data.fp = torch.tensor(fp, dtype=torch.float32).unsqueeze(0)  # [1, fp_dim] for proper batching

        return data
    except Exception:
        return None


def prepare_dataset(
    df: pd.DataFrame,
    featurizer: MoleculeGraphFeaturizer,
    desc: str = "Processing",
) -> List[Data]:
    """Convert DataFrame to list of PyG Data objects."""
    graphs = []

    # Use pIC50 if available, else pValue for legacy data
    pic50_col = 'pIC50' if 'pIC50' in df.columns else 'pValue'

    for _, row in tqdm(df.iterrows(), total=len(df), desc=desc):
        data = create_graph_data(
            row['smiles'],
            row['active'],
            row.get(pic50_col),
            featurizer
        )
        if data is not None:
            graphs.append(data)

    return graphs


def compute_loss(
    outputs: Dict[str, torch.Tensor],
    batch: Batch,
    focal_loss_fn: FocalLoss,
    device: torch.device,
) -> Dict[str, torch.Tensor]:
    """Compute classification + regression loss."""
    losses = {}

    # Classification loss (focal)
    logits = outputs['logits']
    labels = batch.y.to(device).view(-1)

    loss_cls = focal_loss_fn(logits, labels)
    losses['cls'] = loss_cls

    # Regression loss (for compounds with pIC50)
    pIC50_pred = outputs['pIC50']
    pIC50_true = batch.pIC50.to(device).view(-1)

    mask = ~torch.isnan(pIC50_true)
    if mask.any():
        loss_reg = F.mse_loss(pIC50_pred[mask], pIC50_true[mask])
        losses['reg'] = loss_reg
        losses['total'] = loss_cls + 0.3 * loss_reg
    else:
        losses['total'] = loss_cls

    return losses


def train_epoch(
    model: nn.Module,
    loader: PyGDataLoader,
    optimizer: optim.Optimizer,
    focal_loss_fn: FocalLoss,
    device: torch.device,
    scheduler=None,
) -> float:
    """Train for one epoch."""
    model.train()
    total_loss = 0
    n_batches = 0

    for batch in tqdm(loader, desc="Training", leave=False):
        batch = batch.to(device)
        optimizer.zero_grad()

        outputs = model(batch)
        losses = compute_loss(outputs, batch, focal_loss_fn, device)

        loss = losses['total']
        if torch.isnan(loss):
            continue

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        if scheduler:
            scheduler.step()

        total_loss += loss.item()
        n_batches += 1

    return total_loss / max(n_batches, 1)


@torch.no_grad()
def evaluate(
    model: nn.Module,
    loader: PyGDataLoader,
    device: torch.device,
) -> Dict[str, float]:
    """Evaluate model with comprehensive metrics."""
    model.eval()

    all_probs = []
    all_labels = []
    all_pIC50_pred = []
    all_pIC50_true = []

    for batch in loader:
        batch = batch.to(device)
        outputs = model(batch)

        probs = torch.softmax(outputs['logits'], dim=-1)[:, 1]
        all_probs.extend(probs.cpu().numpy())
        all_labels.extend(batch.y.cpu().numpy())

        all_pIC50_pred.extend(outputs['pIC50'].cpu().numpy())
        all_pIC50_true.extend(batch.pIC50.cpu().numpy())

    probs = np.array(all_probs)
    labels = np.array(all_labels)

    metrics = {}

    # ROC-AUC
    if len(np.unique(labels)) > 1:
        metrics['auc'] = roc_auc_score(labels, probs)

    # PR-AUC
    precision, recall, _ = precision_recall_curve(labels, probs)
    metrics['pr_auc'] = auc(recall, precision)

    # Optimal threshold metrics
    thresholds = np.arange(0.3, 0.7, 0.05)
    best_f1 = 0
    best_thresh = 0.5
    for thresh in thresholds:
        preds = (probs >= thresh).astype(int)
        tp = ((preds == 1) & (labels == 1)).sum()
        fp = ((preds == 1) & (labels == 0)).sum()
        fn = ((preds == 0) & (labels == 1)).sum()

        prec = tp / (tp + fp) if (tp + fp) > 0 else 0
        rec = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0

        if f1 > best_f1:
            best_f1 = f1
            best_thresh = thresh

    preds = (probs >= best_thresh).astype(int)
    metrics['accuracy'] = (preds == labels).mean()
    n_pos = labels.sum()
    n_neg = (1 - labels).sum()
    metrics['sensitivity'] = ((preds == 1) & (labels == 1)).sum() / n_pos if n_pos > 0 else 0.0
    metrics['specificity'] = ((preds == 0) & (labels == 0)).sum() / n_neg if n_neg > 0 else 0.0
    metrics['f1'] = best_f1
    metrics['threshold'] = best_thresh

    # pIC50 regression
    pIC50_pred = np.array(all_pIC50_pred)
    pIC50_true = np.array(all_pIC50_true)
    mask = ~np.isnan(pIC50_true)
    if mask.sum() > 10:
        from sklearn.metrics import r2_score, mean_absolute_error
        metrics['pIC50_r2'] = r2_score(pIC50_true[mask], pIC50_pred[mask])
        metrics['pIC50_mae'] = mean_absolute_error(pIC50_true[mask], pIC50_pred[mask])

    return metrics


def main():
    parser = argparse.ArgumentParser(description='hERG Cardiotoxicity Model')
    parser.add_argument('--epochs', type=int, default=100, help='Training epochs')
    parser.add_argument('--batch-size', type=int, default=128, help='Batch size')
    parser.add_argument('--lr', type=float, default=3e-4, help='Learning rate')
    parser.add_argument('--data-dir', type=str, default='data/toxicity', help='Data directory')
    parser.add_argument('--output-dir', type=str, default='models/herg', help='Output directory')
    parser.add_argument('--folds', type=int, default=3, help='Cross-validation folds')
    args = parser.parse_args()

    print("=" * 70)
    print("hERG CARDIOTOXICITY MODEL - MAXIMUM ACCURACY")
    print("=" * 70)
    print(f"Device: {DEVICE}")
    print(f"Epochs: {args.epochs}")
    print(f"Folds: {args.folds}")

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Load data
    print("\n" + "=" * 70)
    print("LOADING DATA")
    print("=" * 70)

    data_dir = Path(args.data_dir)
    df = load_herg_data(data_dir)

    # Deduplicate by SMILES, keep highest pIC50 (strongest binder)
    if 'pIC50' in df.columns:
        df = df.sort_values('pIC50', ascending=False).drop_duplicates(subset=['smiles'], keep='first')
    else:
        df = df.drop_duplicates(subset=['smiles'], keep='first')
    print(f"\nAfter dedup: {len(df)} unique molecules")

    # Balance check
    n_active = (df['active'] == 1).sum()
    n_inactive = (df['active'] == 0).sum()
    print(f"Class balance: {n_active} active ({100*n_active/len(df):.1f}%), {n_inactive} inactive")

    # Create featurizer
    print("\nCreating molecular graphs...")
    featurizer = MoleculeGraphFeaturizer(use_3d=False)
    all_graphs = prepare_dataset(df, featurizer, "Featurizing")
    print(f"Valid graphs: {len(all_graphs)}")

    # Create labels array for stratified split
    labels = np.array([g.y.item() for g in all_graphs])

    # Cross-validation
    print("\n" + "=" * 70)
    print(f"{args.folds}-FOLD CROSS-VALIDATION")
    print("=" * 70)

    skf = StratifiedKFold(n_splits=args.folds, shuffle=True, random_state=42)
    fold_results = []

    for fold, (train_idx, val_idx) in enumerate(skf.split(all_graphs, labels)):
        print(f"\n{'='*70}")
        print(f"FOLD {fold + 1}/{args.folds}")
        print(f"{'='*70}")

        train_graphs = [all_graphs[i] for i in train_idx]
        val_graphs = [all_graphs[i] for i in val_idx]

        print(f"Train: {len(train_graphs)}, Val: {len(val_graphs)}")

        train_loader = PyGDataLoader(train_graphs, batch_size=args.batch_size, shuffle=True)
        val_loader = PyGDataLoader(val_graphs, batch_size=args.batch_size, shuffle=False)

        # Fresh backbone - hERG is K+ channel, not transporter
        backbone = StereoGNN()
        print("Initialized fresh StereoGNN backbone")

        # Create model
        model = HERGClassifier(backbone).to(DEVICE)
        print(f"Parameters: {count_parameters(model):,}")

        # Single LR for all params - full training
        optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)

        # Simple scheduler
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)
        focal_loss = FocalLoss(alpha=0.5, gamma=1.0, pos_weight=1.3)

        # Training loop
        best_auc = 0
        patience = 15
        no_improve = 0

        for epoch in range(1, args.epochs + 1):
            train_loss = train_epoch(model, train_loader, optimizer, focal_loss, DEVICE, None)
            val_metrics = evaluate(model, val_loader, DEVICE)
            scheduler.step(val_metrics.get('auc', 0))

            val_auc = val_metrics.get('auc', 0)
            val_pr = val_metrics.get('pr_auc', 0)

            if epoch % 10 == 0 or val_auc > best_auc:
                print(f"Epoch {epoch:3d} | Loss: {train_loss:.4f} | AUC: {val_auc:.4f} | PR-AUC: {val_pr:.4f}")

            # Checkpoint every 25 epochs
            if epoch % 25 == 0:
                torch.save({
                    'epoch': epoch,
                    'fold': fold,
                    'model_state_dict': model.state_dict(),
                    'val_auc': val_auc,
                }, output_dir / f'checkpoint_fold{fold}_epoch{epoch}.pt')

            if val_auc > best_auc:
                best_auc = val_auc
                no_improve = 0
                torch.save({
                    'epoch': epoch,
                    'fold': fold,
                    'model_state_dict': model.state_dict(),
                    'val_metrics': val_metrics,
                }, output_dir / f'best_fold{fold}.pt')
                print(f"  -> Best model (AUC: {val_auc:.4f}, Sens: {val_metrics.get('sensitivity', 0):.3f}, Spec: {val_metrics.get('specificity', 0):.3f})")
            else:
                no_improve += 1
                if no_improve >= patience:
                    print(f"Early stopping at epoch {epoch}")
                    break

        # Load best model for this fold and get final metrics
        best_ckpt = torch.load(output_dir / f'best_fold{fold}.pt', map_location=DEVICE, weights_only=False)
        model.load_state_dict(best_ckpt['model_state_dict'])
        final_metrics = evaluate(model, val_loader, DEVICE)
        fold_results.append(final_metrics)

        print(f"\nFold {fold + 1} Final: AUC={final_metrics['auc']:.4f}, PR-AUC={final_metrics['pr_auc']:.4f}")

    # Aggregate results
    print("\n" + "=" * 70)
    print("CROSS-VALIDATION RESULTS")
    print("=" * 70)

    for metric in ['auc', 'pr_auc', 'accuracy', 'sensitivity', 'specificity', 'f1']:
        values = [r.get(metric, 0) for r in fold_results]
        print(f"{metric:15s}: {np.mean(values):.4f} +/- {np.std(values):.4f}")

    # Save results
    results = {
        'fold_results': fold_results,
        'mean_auc': np.mean([r['auc'] for r in fold_results]),
        'std_auc': np.std([r['auc'] for r in fold_results]),
        'mean_pr_auc': np.mean([r['pr_auc'] for r in fold_results]),
        'n_molecules': len(all_graphs),
        'n_active': int(labels.sum()),
        'timestamp': datetime.now().isoformat(),
    }

    with open(output_dir / 'results.json', 'w') as f:
        json.dump(results, f, indent=2, default=float)

    print("\n" + "=" * 70)
    print("TRAINING COMPLETE")
    print(f"Mean AUC: {results['mean_auc']:.4f} +/- {results['std_auc']:.4f}")
    print(f"Models saved to: {output_dir}")
    print("=" * 70)


if __name__ == "__main__":
    main()
